{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\" \n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids(physiology_folder)\n",
    "\n",
    "splits = split_subjects_train_test(subjects, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xochipilli\\AppData\\Local\\Temp\\ipykernel_8464\\1364739470.py:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "import json\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, lstm_units=64, dropout_rate=0.4, kernel_regularizer_l1=0.0032):\n",
    "    input_signal = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(16, 5, padding='same')(input_signal)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = LSTM(lstm_units)(x)\n",
    "\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l1(kernel_regularizer_l1))(x)\n",
    "\n",
    "    return Model(inputs=input_signal, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "from multiprocessing import Value\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology\\sub_0_vid_0.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology\\sub_0_vid_10.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology\\sub_0_vid_11.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology\\sub_0_vid_13.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology\\sub_0_vid_14.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology\\sub_0_vid_2.csv\n"
     ]
    }
   ],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\" \n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids(physiology_folder)\n",
    "\n",
    "num_cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "def process_files(annotation_file, physiology_file,):\n",
    "    df_annotations = pd.read_csv(annotation_file)\n",
    "    df_physiology = pd.read_csv(physiology_file)\n",
    "    \n",
    "    print(physiology_file)\n",
    "    X, y, numeric_column_indices = preprocess(df_physiology, df_annotations, predictions_cols=['arousal','valence'], aggregate=None, window=[-10000, 10000], partition_window = 1)\n",
    "    # print(X.shape, y.shape)\n",
    "    \n",
    "    save_files(X, y, annotation_file, phys_folder, ann_folder)\n",
    "    \n",
    "    return None\n",
    "\n",
    "## Process the files using the context manager\n",
    "#with parallel_backend('multiprocessing', n_jobs=num_cpu_cores // 3):\n",
    "#    with tqdm_joblib(tqdm(total=len(zipped_files), desc=\"Files\", leave=False)) as progress_bar:\n",
    "#        results = Parallel()(\n",
    "#            (delayed(process_files)(ann_file, phys_file) for ann_file, phys_file in zipped_files)\n",
    "#        )\n",
    "#\n",
    "for ann_file, phys_file in zipped_files:\n",
    "    process_files(ann_file, phys_file,)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   0%|\u001b[32m          \u001b[0m| 0/8 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 0\n",
      "X_test shape: (20543, 1000, 15)\n",
      "y_test shape: (20543, 2)\n",
      "1028/1028 [==============================] - 28s 26ms/step - loss: 6.0823 - valence_output_loss: 1.7589 - arousal_output_loss: 2.2067 - valence_output_root_mean_squared_error: 1.3263 - arousal_output_root_mean_squared_error: 1.4855 - val_loss: 8.6807 - val_valence_output_loss: 3.9136 - val_arousal_output_loss: 4.1343 - val_valence_output_root_mean_squared_error: 1.9783 - val_arousal_output_root_mean_squared_error: 2.0333\n",
      "642/642 [==============================] - 5s 7ms/step - loss: 15.0058 - valence_output_loss: 8.2676 - arousal_output_loss: 6.1055 - valence_output_root_mean_squared_error: 2.8753 - arousal_output_root_mean_squared_error: 2.4709\n",
      "X_test shape: (20542, 1000, 15)\n",
      "y_test shape: (20542, 2)\n",
      "1028/1028 [==============================] - 41s 39ms/step - loss: 7.0729 - valence_output_loss: 2.8235 - arousal_output_loss: 2.5606 - valence_output_root_mean_squared_error: 1.6803 - arousal_output_root_mean_squared_error: 1.6002 - val_loss: 12.5680 - val_valence_output_loss: 10.3470 - val_arousal_output_loss: 1.6572 - val_valence_output_root_mean_squared_error: 3.2167 - val_arousal_output_root_mean_squared_error: 1.2873\n",
      "642/642 [==============================] - 8s 12ms/step - loss: 16.2454 - valence_output_loss: 9.0707 - arousal_output_loss: 6.6110 - valence_output_root_mean_squared_error: 3.0118 - arousal_output_root_mean_squared_error: 2.5712\n",
      "X_test shape: (20543, 1000, 15)\n",
      "y_test shape: (20543, 2)\n",
      "1028/1028 [==============================] - 40s 38ms/step - loss: 7.0300 - valence_output_loss: 3.2274 - arousal_output_loss: 1.8322 - valence_output_root_mean_squared_error: 1.7965 - arousal_output_root_mean_squared_error: 1.3536 - val_loss: 10.0189 - val_valence_output_loss: 5.7646 - val_arousal_output_loss: 3.5370 - val_valence_output_root_mean_squared_error: 2.4010 - val_arousal_output_root_mean_squared_error: 1.8807\n",
      "642/642 [==============================] - 7s 11ms/step - loss: 6.7567 - valence_output_loss: 2.2828 - arousal_output_loss: 3.7566 - valence_output_root_mean_squared_error: 1.5109 - arousal_output_root_mean_squared_error: 1.9382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  12%|\u001b[32m█▎        \u001b[0m| 1/8 [02:14<15:42, 134.61s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 10\n",
      "X_test shape: (12240, 1000, 15)\n",
      "y_test shape: (12240, 2)\n",
      "612/612 [==============================] - 22s 35ms/step - loss: 4.9904 - valence_output_loss: 2.0081 - arousal_output_loss: 0.6466 - valence_output_root_mean_squared_error: 1.4171 - arousal_output_root_mean_squared_error: 0.8041 - val_loss: 2.5988 - val_valence_output_loss: 0.6185 - val_arousal_output_loss: 1.3101 - val_valence_output_root_mean_squared_error: 0.7865 - val_arousal_output_root_mean_squared_error: 1.1446\n",
      "383/383 [==============================] - 4s 9ms/step - loss: 8.1449 - valence_output_loss: 2.6057 - arousal_output_loss: 4.8690 - valence_output_root_mean_squared_error: 1.6142 - arousal_output_root_mean_squared_error: 2.2066\n",
      "X_test shape: (12240, 1000, 15)\n",
      "y_test shape: (12240, 2)\n",
      "612/612 [==============================] - 19s 29ms/step - loss: 7.2566 - valence_output_loss: 1.5577 - arousal_output_loss: 3.2209 - valence_output_root_mean_squared_error: 1.2481 - arousal_output_root_mean_squared_error: 1.7947 - val_loss: 36.1595 - val_valence_output_loss: 25.1269 - val_arousal_output_loss: 10.4549 - val_valence_output_root_mean_squared_error: 5.0127 - val_arousal_output_root_mean_squared_error: 3.2334\n",
      "383/383 [==============================] - 3s 8ms/step - loss: 25.9670 - valence_output_loss: 10.7590 - arousal_output_loss: 14.6302 - valence_output_root_mean_squared_error: 3.2801 - arousal_output_root_mean_squared_error: 3.8249\n",
      "X_test shape: (12240, 1000, 15)\n",
      "y_test shape: (12240, 2)\n",
      "612/612 [==============================] - 18s 28ms/step - loss: 6.1419 - valence_output_loss: 2.5176 - arousal_output_loss: 0.8369 - valence_output_root_mean_squared_error: 1.5867 - arousal_output_root_mean_squared_error: 0.9148 - val_loss: 13.0884 - val_valence_output_loss: 8.8700 - val_arousal_output_loss: 3.2748 - val_valence_output_root_mean_squared_error: 2.9783 - val_arousal_output_root_mean_squared_error: 1.8097\n",
      "383/383 [==============================] - 3s 8ms/step - loss: 22.0859 - valence_output_loss: 7.9833 - arousal_output_loss: 13.1590 - valence_output_root_mean_squared_error: 2.8255 - arousal_output_root_mean_squared_error: 3.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  25%|\u001b[32m██▌       \u001b[0m| 2/8 [03:26<09:47, 97.85s/video] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 11\n",
      "X_test shape: (8008, 1000, 15)\n",
      "y_test shape: (8008, 2)\n",
      "401/401 [==============================] - 12s 28ms/step - loss: 4.5879 - valence_output_loss: 0.2773 - arousal_output_loss: 1.5303 - valence_output_root_mean_squared_error: 0.5266 - arousal_output_root_mean_squared_error: 1.2371 - val_loss: 1.1994 - val_valence_output_loss: 0.3152 - val_arousal_output_loss: 0.1540 - val_valence_output_root_mean_squared_error: 0.5614 - val_arousal_output_root_mean_squared_error: 0.3924\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 4.1803 - valence_output_loss: 1.6414 - arousal_output_loss: 1.8087 - valence_output_root_mean_squared_error: 1.2812 - arousal_output_root_mean_squared_error: 1.3449\n",
      "X_test shape: (8008, 1000, 15)\n",
      "y_test shape: (8008, 2)\n",
      "401/401 [==============================] - 12s 28ms/step - loss: 15.5525 - valence_output_loss: 6.8632 - arousal_output_loss: 5.7106 - valence_output_root_mean_squared_error: 2.6198 - arousal_output_root_mean_squared_error: 2.3897 - val_loss: 1.7205 - val_valence_output_loss: 0.1955 - val_arousal_output_loss: 0.3081 - val_valence_output_root_mean_squared_error: 0.4421 - val_arousal_output_root_mean_squared_error: 0.5551\n",
      "251/251 [==============================] - 2s 8ms/step - loss: 2.4290 - valence_output_loss: 0.2121 - arousal_output_loss: 1.0001 - valence_output_root_mean_squared_error: 0.4605 - arousal_output_root_mean_squared_error: 1.0000\n",
      "X_test shape: (8008, 1000, 15)\n",
      "y_test shape: (8008, 2)\n",
      "401/401 [==============================] - 14s 34ms/step - loss: 6.8156 - valence_output_loss: 2.3953 - arousal_output_loss: 1.0612 - valence_output_root_mean_squared_error: 1.5477 - arousal_output_root_mean_squared_error: 1.0301 - val_loss: 5.5882 - val_valence_output_loss: 2.0757 - val_arousal_output_loss: 2.3679 - val_valence_output_root_mean_squared_error: 1.4407 - val_arousal_output_root_mean_squared_error: 1.5388\n",
      "251/251 [==============================] - 6s 22ms/step - loss: 4.3436 - valence_output_loss: 2.0146 - arousal_output_loss: 1.1845 - valence_output_root_mean_squared_error: 1.4194 - arousal_output_root_mean_squared_error: 1.0883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  38%|\u001b[32m███▊      \u001b[0m| 3/8 [04:18<06:23, 76.65s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 13\n",
      "X_test shape: (16716, 1000, 15)\n",
      "y_test shape: (16716, 2)\n",
      "836/836 [==============================] - 43s 50ms/step - loss: 2.2101 - valence_output_loss: 0.5004 - arousal_output_loss: 0.3080 - valence_output_root_mean_squared_error: 0.7074 - arousal_output_root_mean_squared_error: 0.5550 - val_loss: 0.9116 - val_valence_output_loss: 0.4858 - val_arousal_output_loss: 0.2558 - val_valence_output_root_mean_squared_error: 0.6970 - val_arousal_output_root_mean_squared_error: 0.5058\n",
      "523/523 [==============================] - 9s 16ms/step - loss: 7.6835 - valence_output_loss: 6.1521 - arousal_output_loss: 1.3613 - valence_output_root_mean_squared_error: 2.4803 - arousal_output_root_mean_squared_error: 1.1667\n",
      "X_test shape: (16715, 1000, 15)\n",
      "y_test shape: (16715, 2)\n",
      "836/836 [==============================] - 40s 47ms/step - loss: 8.6422 - valence_output_loss: 3.1153 - arousal_output_loss: 3.6843 - valence_output_root_mean_squared_error: 1.7650 - arousal_output_root_mean_squared_error: 1.9195 - val_loss: 5.7720 - val_valence_output_loss: 2.9293 - val_arousal_output_loss: 2.5998 - val_valence_output_root_mean_squared_error: 1.7115 - val_arousal_output_root_mean_squared_error: 1.6124\n",
      "523/523 [==============================] - 7s 13ms/step - loss: 28.5001 - valence_output_loss: 10.9309 - arousal_output_loss: 17.3263 - valence_output_root_mean_squared_error: 3.3062 - arousal_output_root_mean_squared_error: 4.1625\n",
      "X_test shape: (16714, 1000, 15)\n",
      "y_test shape: (16714, 2)\n",
      "836/836 [==============================] - 40s 47ms/step - loss: 7.0091 - valence_output_loss: 3.9007 - arousal_output_loss: 0.8094 - valence_output_root_mean_squared_error: 1.9750 - arousal_output_root_mean_squared_error: 0.8997 - val_loss: 5.9876 - val_valence_output_loss: 0.8076 - val_arousal_output_loss: 4.6451 - val_valence_output_root_mean_squared_error: 0.8987 - val_arousal_output_root_mean_squared_error: 2.1552\n",
      "523/523 [==============================] - 6s 12ms/step - loss: 2.1054 - valence_output_loss: 1.0141 - arousal_output_loss: 0.5564 - valence_output_root_mean_squared_error: 1.0070 - arousal_output_root_mean_squared_error: 0.7459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  50%|\u001b[32m█████     \u001b[0m| 4/8 [06:47<07:01, 105.39s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 14\n",
      "X_test shape: (14640, 1000, 15)\n",
      "y_test shape: (14640, 2)\n",
      "732/732 [==============================] - 25s 33ms/step - loss: 3.0098 - valence_output_loss: 0.5357 - arousal_output_loss: 0.6170 - valence_output_root_mean_squared_error: 0.7319 - arousal_output_root_mean_squared_error: 0.7855 - val_loss: 1.8486 - val_valence_output_loss: 0.6350 - val_arousal_output_loss: 0.8017 - val_valence_output_root_mean_squared_error: 0.7969 - val_arousal_output_root_mean_squared_error: 0.8954\n",
      "458/458 [==============================] - 5s 10ms/step - loss: 4.7233 - valence_output_loss: 0.9734 - arousal_output_loss: 3.3380 - valence_output_root_mean_squared_error: 0.9866 - arousal_output_root_mean_squared_error: 1.8270\n",
      "X_test shape: (14640, 1000, 15)\n",
      "y_test shape: (14640, 2)\n",
      "732/732 [==============================] - 25s 33ms/step - loss: 3.9870 - valence_output_loss: 0.7205 - arousal_output_loss: 0.9693 - valence_output_root_mean_squared_error: 0.8488 - arousal_output_root_mean_squared_error: 0.9845 - val_loss: 3.2066 - val_valence_output_loss: 0.2069 - val_arousal_output_loss: 2.4479 - val_valence_output_root_mean_squared_error: 0.4548 - val_arousal_output_root_mean_squared_error: 1.5646\n",
      "458/458 [==============================] - 4s 9ms/step - loss: 30.5373 - valence_output_loss: 18.0640 - arousal_output_loss: 11.9215 - valence_output_root_mean_squared_error: 4.2502 - arousal_output_root_mean_squared_error: 3.4528\n",
      "X_test shape: (14640, 1000, 15)\n",
      "y_test shape: (14640, 2)\n",
      "732/732 [==============================] - 25s 33ms/step - loss: 4.2186 - valence_output_loss: 0.5958 - arousal_output_loss: 1.4807 - valence_output_root_mean_squared_error: 0.7719 - arousal_output_root_mean_squared_error: 1.2168 - val_loss: 1.7034 - val_valence_output_loss: 0.7093 - val_arousal_output_loss: 0.3814 - val_valence_output_root_mean_squared_error: 0.8422 - val_arousal_output_root_mean_squared_error: 0.6176\n",
      "458/458 [==============================] - 5s 10ms/step - loss: 2.7560 - valence_output_loss: 0.1176 - arousal_output_loss: 2.0258 - valence_output_root_mean_squared_error: 0.3429 - arousal_output_root_mean_squared_error: 1.4233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  62%|\u001b[32m██████▎   \u001b[0m| 5/8 [08:20<05:02, 100.76s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 2\n",
      "X_test shape: (12024, 1000, 15)\n",
      "y_test shape: (12024, 2)\n",
      "602/602 [==============================] - 22s 35ms/step - loss: 3.8191 - valence_output_loss: 0.9358 - arousal_output_loss: 0.6356 - valence_output_root_mean_squared_error: 0.9674 - arousal_output_root_mean_squared_error: 0.7973 - val_loss: 21.0557 - val_valence_output_loss: 11.8469 - val_arousal_output_loss: 8.6730 - val_valence_output_root_mean_squared_error: 3.4419 - val_arousal_output_root_mean_squared_error: 2.9450\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 22.5493 - valence_output_loss: 12.5103 - arousal_output_loss: 9.5032 - valence_output_root_mean_squared_error: 3.5370 - arousal_output_root_mean_squared_error: 3.0827\n",
      "X_test shape: (12023, 1000, 15)\n",
      "y_test shape: (12023, 2)\n",
      "602/602 [==============================] - 51s 83ms/step - loss: 4.1581 - valence_output_loss: 0.7506 - arousal_output_loss: 0.9697 - valence_output_root_mean_squared_error: 0.8664 - arousal_output_root_mean_squared_error: 0.9848 - val_loss: 1.7465 - val_valence_output_loss: 0.1343 - val_arousal_output_loss: 1.0518 - val_valence_output_root_mean_squared_error: 0.3664 - val_arousal_output_root_mean_squared_error: 1.0256\n",
      "376/376 [==============================] - 6s 16ms/step - loss: 4.6532 - valence_output_loss: 2.4710 - arousal_output_loss: 1.6217 - valence_output_root_mean_squared_error: 1.5719 - arousal_output_root_mean_squared_error: 1.2735\n",
      "X_test shape: (12023, 1000, 15)\n",
      "y_test shape: (12023, 2)\n",
      "602/602 [==============================] - 40s 65ms/step - loss: 5.6051 - valence_output_loss: 1.9535 - arousal_output_loss: 0.9650 - valence_output_root_mean_squared_error: 1.3977 - arousal_output_root_mean_squared_error: 0.9824 - val_loss: 3.2773 - val_valence_output_loss: 1.3042 - val_arousal_output_loss: 0.9039 - val_valence_output_root_mean_squared_error: 1.1420 - val_arousal_output_root_mean_squared_error: 0.9507\n",
      "376/376 [==============================] - 6s 15ms/step - loss: 24.4146 - valence_output_loss: 14.4553 - arousal_output_loss: 8.8901 - valence_output_root_mean_squared_error: 3.8020 - arousal_output_root_mean_squared_error: 2.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  75%|\u001b[32m███████▌  \u001b[0m| 6/8 [10:32<03:43, 111.65s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 20\n",
      "X_test shape: (18715, 1000, 15)\n",
      "y_test shape: (18715, 2)\n",
      "936/936 [==============================] - 53s 56ms/step - loss: 3.4524 - valence_output_loss: 1.5072 - arousal_output_loss: 0.3573 - valence_output_root_mean_squared_error: 1.2277 - arousal_output_root_mean_squared_error: 0.5977 - val_loss: 14.8903 - val_valence_output_loss: 9.4542 - val_arousal_output_loss: 4.9521 - val_valence_output_root_mean_squared_error: 3.0748 - val_arousal_output_root_mean_squared_error: 2.2253\n",
      "585/585 [==============================] - 8s 14ms/step - loss: 23.2956 - valence_output_loss: 10.5440 - arousal_output_loss: 12.2675 - valence_output_root_mean_squared_error: 3.2472 - arousal_output_root_mean_squared_error: 3.5025\n",
      "X_test shape: (18716, 1000, 15)\n",
      "y_test shape: (18716, 2)\n",
      "936/936 [==============================] - 45s 47ms/step - loss: 3.3379 - valence_output_loss: 1.0412 - arousal_output_loss: 0.4721 - valence_output_root_mean_squared_error: 1.0204 - arousal_output_root_mean_squared_error: 0.6871 - val_loss: 2.6090 - val_valence_output_loss: 1.6067 - val_arousal_output_loss: 0.5598 - val_valence_output_root_mean_squared_error: 1.2675 - val_arousal_output_root_mean_squared_error: 0.7482\n",
      "585/585 [==============================] - 7s 12ms/step - loss: 19.6993 - valence_output_loss: 18.7427 - arousal_output_loss: 0.5141 - valence_output_root_mean_squared_error: 4.3293 - arousal_output_root_mean_squared_error: 0.7170\n",
      "X_test shape: (18715, 1000, 15)\n",
      "y_test shape: (18715, 2)\n",
      "772/936 [=======================>......] - ETA: 7s - loss: 6.7447 - valence_output_loss: 3.1402 - arousal_output_loss: 0.8857 - valence_output_root_mean_squared_error: 1.7721 - arousal_output_root_mean_squared_error: 0.9411"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor\n",
    "import threading\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "# Initialize an empty dictionary to store the fold results\n",
    "fold_results = {}\n",
    "\n",
    "# Loop through the splits\n",
    "for split_index, split in enumerate(splits):\n",
    "\n",
    "        X_train, X_test, y_train, y_test, numeric_column_indices, categorical_column_indices = preprocess(\n",
    "            df_physiology_video.copy(), df_annotations_video.copy(), split=split, predictions_cols=['arousal', 'valence'], aggregate=None,\n",
    "            window_duration=10000, resample_rate=100)\n",
    "\n",
    "        print(\"X_test shape:\", X_test.shape)\n",
    "        print(\"y_test shape:\", y_test.shape)\n",
    "                \n",
    "        # Extract arousal and valence values from y_train and y_test\n",
    "        y_arousal_train = y_train[:, 0]\n",
    "        y_valence_train = y_train[:, 1]\n",
    "        y_arousal_test = y_test[:, 0]\n",
    "        y_valence_test = y_test[:, 1]\n",
    "\n",
    "        # Normalize the input data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        # Define a Keras wrapper for the model\n",
    "        def model_builder(hp):\n",
    "            lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "            dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "            kernel_regularizer_l1 = hp.Choice('kernel_regularizer_l1', values=[0.001, 0.01, 0.1])\n",
    "\n",
    "            model = create_cnn_lstm_model(input_shape, lstm_units, dropout_rate, kernel_regularizer_l1)\n",
    "\n",
    "            valence_output = Dense(1, activation='sigmoid')(model.output)\n",
    "            valence_output = Lambda(lambda x: x * 8 + 1, name='valence_output')(valence_output)\n",
    "            \n",
    "            arousal_output = Dense(1, activation='sigmoid')(model.output)\n",
    "            arousal_output = Lambda(lambda x: x * 8 + 1, name='arousal_output')(arousal_output)\n",
    "\n",
    "            # Combine the model\n",
    "            final_model = Model(inputs=model.input, outputs=[valence_output, arousal_output])\n",
    "\n",
    "            # Compile the model\n",
    "            final_model.compile(optimizer='adam',\n",
    "                                loss={'valence_output': 'mse',\n",
    "                                    'arousal_output': 'mse'},\n",
    "                                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "            return final_model\n",
    "\n",
    "        # Hyperparameter search\n",
    "        tuner = RandomSearch(\n",
    "            model_builder,\n",
    "            objective='val_loss',\n",
    "            max_trials=5,\n",
    "            executions_per_trial=3,\n",
    "            directory='random_search',\n",
    "            project_name='emotion_recognition'\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "        tuner.search(X_train, {'valence_output': y_valence_train, 'arousal_output': y_arousal_train},\n",
    "                    validation_split=0.2, epochs=1, batch_size=32,\n",
    "                    callbacks=[early_stopping_callback])\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "        # Train the best model\n",
    "        history = best_model.fit(X_train, {'valence_output': y_valence_train, 'arousal_output': y_arousal_train},\n",
    "                                validation_split=0.2, epochs=50, batch_size=32,\n",
    "                                callbacks=[early_stopping_callback])\n",
    "\n",
    "        evaluation = best_model.evaluate(X_test, [y_valence_test, y_arousal_test])\n",
    "\n",
    "        # Save the evaluation results in the dictionary\n",
    "        key = f\"video_{video}_split_{split_index}\"\n",
    "        results[key] = {\n",
    "            'valence_output_root_mean_squared_error': evaluation[3],\n",
    "            'arousal_output_root_mean_squared_error': evaluation[4]\n",
    "        }\n",
    "\n",
    "    # Calculate the average of the evaluation results\n",
    "    evaluation_sum = np.array([list(rmse_dict.values()) for rmse_dict in results.values()]).sum(axis=0)\n",
    "    average_evaluation = evaluation_sum / len(splits)\n",
    "\n",
    "    # Save the results for the current video\n",
    "    video_results[f\"video_{video}\"] = results\n",
    "\n",
    "    # Save the average evaluation in the dictionary\n",
    "    video_results[f\"video_{video}_average\"] = {\n",
    "        'valence_output_root_mean_squared_error': average_evaluation[0],\n",
    "        'arousal_output_root_mean_squared_error': average_evaluation[1]\n",
    "    }\n",
    "\n",
    "    # Reset the results dictionary for the next video\n",
    "    results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for each split:\n",
      "{'video_0_subject_13_split_0': [409.2861328125, 232.13124084472656, 176.3783416748047, 15.235854148864746, 13.28075122833252], 'video_0_subject_34_split_1': [9.059562683105469, 4.963016510009766, 2.896031618118286, 2.227782964706421, 1.7017730474472046], 'video_0_subject_43_split_2': [30.761016845703125, 7.859403133392334, 21.74477767944336, 2.8034627437591553, 4.663129806518555], 'average': array([149.70223745,  81.65122016,  67.00638366,   6.75569995,\n",
      "         6.54855136])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for each split:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642/642 [==============================] - 5s 8ms/step - loss: 30.7610 - valence_output_loss: 7.8594 - arousal_output_loss: 21.7448 - valence_output_root_mean_squared_error: 2.8035 - arousal_output_root_mean_squared_error: 4.6631\n",
      "[30.761016845703125, 7.859403133392334, 21.74477767944336, 2.8034627437591553, 4.663129806518555]\n"
     ]
    }
   ],
   "source": [
    "print(final_model.evaluate(X_test, [y_valence_test, y_arousal_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1000, 41085, 15)\n",
      "y_valence_train shape: (41085,)\n",
      "y_arousal_train shape: (41085,)\n",
      "X_test shape: (1000, 20543, 15)\n",
      "y_valence_test shape: (20543,)\n",
      "y_arousal_test shape: (20543,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_valence_train shape:\", y_valence_train.shape)\n",
    "print(\"y_arousal_train shape:\", y_arousal_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_valence_test shape:\", y_valence_test.shape)\n",
    "print(\"y_arousal_test shape:\", y_arousal_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': [34, 43, 31, 45, 32, 9, 2, 11, 27, 26, 18, 28, 7, 29, 5, 41],\n",
       "  'test': array([13,  0,  1, 20, 17, 15, 30, 24], dtype=int64)},\n",
       " {'train': [13, 43, 0, 45, 1, 9, 20, 11, 17, 26, 15, 28, 30, 29, 24, 41],\n",
       "  'test': array([34, 31, 32,  2, 27, 18,  7,  5], dtype=int64)},\n",
       " {'train': [13, 34, 0, 31, 1, 32, 20, 2, 17, 27, 15, 18, 30, 7, 24, 5],\n",
       "  'test': array([43, 45,  9, 11, 26, 28, 29, 41], dtype=int64)}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[split for split_index, split in enumerate(splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.swapaxes(X_test, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 41085, 15), found shape=(None, 1000, 15)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(final_model\u001b[39m.\u001b[39;49mevaluate(X_test, [y_valence_test, y_arousal_test]))\n",
      "File \u001b[1;32mc:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\XOCHIP~1\\AppData\\Local\\Temp\\__autograph_generated_file4yog_oo1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\xochipilli\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 41085, 15), found shape=(None, 1000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(final_model.evaluate(X_test, [y_valence_test, y_arousal_test]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20543, 15)\n",
      "(20543,)\n",
      "(20543,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_valence_test.shape)\n",
    "print(y_arousal_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
