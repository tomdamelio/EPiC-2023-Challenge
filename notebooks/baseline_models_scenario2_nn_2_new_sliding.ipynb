{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xochipilli\\AppData\\Local\\Temp\\ipykernel_11972\\1464098886.py:35: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "from multiprocessing import Value\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *\n",
    "\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "import json\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21237d8661647819e7746fc0e5e93e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\" \n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids(physiology_folder)\n",
    "\n",
    "num_cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "def process_files(annotation_file, physiology_file,):\n",
    "    df_annotations = pd.read_csv(annotation_file)\n",
    "    df_physiology = pd.read_csv(physiology_file)\n",
    "    \n",
    "    print(physiology_file)\n",
    "    X, y, numeric_column_indices = preprocess(df_physiology, df_annotations, predictions_cols=['arousal','valence'], aggregate=None, window=[-10000, 10000], partition_window = 1)\n",
    "    # print(X.shape, y.shape)\n",
    "    \n",
    "    save_files(X, y, annotation_file, phys_folder, ann_folder)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# zipped_files = zipped_files[115:]\n",
    "\n",
    "## Process the files using the context manager\n",
    "with parallel_backend('loky', n_jobs=num_cpu_cores // 3):\n",
    "    with tqdm_joblib(tqdm(total=len(zipped_files), desc=\"Files\", leave=False)) as progress_bar:\n",
    "        results = Parallel()(\n",
    "            (delayed(process_files)(ann_file, phys_file) for ann_file, phys_file in zipped_files)\n",
    "        )\n",
    "\n",
    "#for ann_file, phys_file in zipped_files:\n",
    "#    process_files(ann_file, phys_file,)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phys_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\" \n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids(physiology_folder)\n",
    "\n",
    "# Comment this lineas\n",
    "videos = [0]\n",
    "subjects = [0,1,11]\n",
    "\n",
    "splits = split_subjects_train_test(subjects, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cnn_lstm_model(input_shape, lstm_units=64, dropout_rate=0.4, kernel_regularizer_l1=0.0032):\n",
    "    input_signal = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(16, 5, padding='same')(input_signal)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = LSTM(lstm_units)(x)\n",
    "\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l1(kernel_regularizer_l1))(x)\n",
    "\n",
    "    return Model(inputs=input_signal, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[split for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    X_train, X_test = load_and_concatenate_files(phys_folder, split, videos)\n",
    "    y_train, y_test = load_and_concatenate_files(ann_folder, split, videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(phys_folder, f\"sub_0_vid_9.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(file_path):\n",
    "    data = np.load(file_path)\n",
    "    train_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cc = np.concatenate(train_data, axis=1) if train_data else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_test, subs in train_test_split.items():\n",
    "    if type(vid) == 'list':\n",
    "        for v in vid:\n",
    "            for sub in subs:\n",
    "                file_path = os.path.join(base_path, f\"sub_{sub}_vid_{v}.npy\")\n",
    "                if os.path.exists(file_path):\n",
    "                    data = np.load(file_path)\n",
    "                    if train_test == \"train\":\n",
    "                        train_data.append(data)\n",
    "                    else:\n",
    "                        test_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    for train_test, subs in train_test_split.items():\n",
    "        if type(vid) == 'list':\n",
    "            for v in vid:\n",
    "                for sub in subs:\n",
    "                    file_path = os.path.join(base_path, f\"sub_{sub}_vid_{v}.npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        data = np.load(file_path)\n",
    "                        if train_test == \"train\":\n",
    "                            train_data.append(data)\n",
    "                        else:\n",
    "                            test_data.append(data)\n",
    "\n",
    "        else:\n",
    "            for sub in subs:\n",
    "                file_path = os.path.join(base_path, f\"sub_{sub}_vid_{vid}.npy\")\n",
    "                if os.path.exists(file_path):\n",
    "                    data = np.load(file_path)\n",
    "                    if train_test == \"train\":\n",
    "                        train_data.append(data)\n",
    "                    else:\n",
    "                        test_data.append(data)\n",
    "\n",
    "    train_data = np.concatenate(train_data) if train_data else None\n",
    "    test_data = np.concatenate(test_data) if test_data else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor\n",
    "import threading\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "# Initialize an empty dictionary to store the fold results\n",
    "fold_results = {}\n",
    "\n",
    "# Loop through the splits\n",
    "for split_index, split in enumerate(splits):\n",
    "\n",
    "        X_train, X_test, y_train, y_test, numeric_column_indices, categorical_column_indices = preprocess(\n",
    "            df_physiology_video.copy(), df_annotations_video.copy(), split=split, predictions_cols=['arousal', 'valence'], aggregate=None,\n",
    "            window_duration=10000, resample_rate=100)\n",
    "\n",
    "        print(\"X_test shape:\", X_test.shape)\n",
    "        print(\"y_test shape:\", y_test.shape)\n",
    "                \n",
    "        # Extract arousal and valence values from y_train and y_test\n",
    "        y_arousal_train = y_train[:, 0]\n",
    "        y_valence_train = y_train[:, 1]\n",
    "        y_arousal_test = y_test[:, 0]\n",
    "        y_valence_test = y_test[:, 1]\n",
    "\n",
    "        # Normalize the input data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        # Define a Keras wrapper for the model\n",
    "        def model_builder(hp):\n",
    "            lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "            dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "            kernel_regularizer_l1 = hp.Choice('kernel_regularizer_l1', values=[0.001, 0.01, 0.1])\n",
    "\n",
    "            model = create_cnn_lstm_model(input_shape, lstm_units, dropout_rate, kernel_regularizer_l1)\n",
    "\n",
    "            valence_output = Dense(1, activation='sigmoid')(model.output)\n",
    "            valence_output = Lambda(lambda x: x * 8 + 1, name='valence_output')(valence_output)\n",
    "            \n",
    "            arousal_output = Dense(1, activation='sigmoid')(model.output)\n",
    "            arousal_output = Lambda(lambda x: x * 8 + 1, name='arousal_output')(arousal_output)\n",
    "\n",
    "            # Combine the model\n",
    "            final_model = Model(inputs=model.input, outputs=[valence_output, arousal_output])\n",
    "\n",
    "            # Compile the model\n",
    "            final_model.compile(optimizer='adam',\n",
    "                                loss={'valence_output': 'mse',\n",
    "                                    'arousal_output': 'mse'},\n",
    "                                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "            return final_model\n",
    "\n",
    "        # Hyperparameter search\n",
    "        tuner = RandomSearch(\n",
    "            model_builder,\n",
    "            objective='val_loss',\n",
    "            max_trials=5,\n",
    "            executions_per_trial=3,\n",
    "            directory='random_search',\n",
    "            project_name='emotion_recognition'\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "        tuner.search(X_train, {'valence_output': y_valence_train, 'arousal_output': y_arousal_train},\n",
    "                    validation_split=0.2, epochs=1, batch_size=32,\n",
    "                    callbacks=[early_stopping_callback])\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "        # Train the best model\n",
    "        history = best_model.fit(X_train, {'valence_output': y_valence_train, 'arousal_output': y_arousal_train},\n",
    "                                validation_split=0.2, epochs=50, batch_size=32,\n",
    "                                callbacks=[early_stopping_callback])\n",
    "\n",
    "        evaluation = best_model.evaluate(X_test, [y_valence_test, y_arousal_test])\n",
    "\n",
    "        # Save the evaluation results in the dictionary\n",
    "        key = f\"video_{video}_split_{split_index}\"\n",
    "        results[key] = {\n",
    "            'valence_output_root_mean_squared_error': evaluation[3],\n",
    "            'arousal_output_root_mean_squared_error': evaluation[4]\n",
    "        }\n",
    "\n",
    "    # Calculate the average of the evaluation results\n",
    "    evaluation_sum = np.array([list(rmse_dict.values()) for rmse_dict in results.values()]).sum(axis=0)\n",
    "    average_evaluation = evaluation_sum / len(splits)\n",
    "\n",
    "    # Save the results for the current video\n",
    "    video_results[f\"video_{video}\"] = results\n",
    "\n",
    "    # Save the average evaluation in the dictionary\n",
    "    video_results[f\"video_{video}_average\"] = {\n",
    "        'valence_output_root_mean_squared_error': average_evaluation[0],\n",
    "        'arousal_output_root_mean_squared_error': average_evaluation[1]\n",
    "    }\n",
    "\n",
    "    # Reset the results dictionary for the next video\n",
    "    results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for each split:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model.evaluate(X_test, [y_valence_test, y_arousal_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_valence_train shape:\", y_valence_train.shape)\n",
    "print(\"y_arousal_train shape:\", y_arousal_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_valence_test shape:\", y_valence_test.shape)\n",
    "print(\"y_arousal_test shape:\", y_arousal_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[split for split_index, split in enumerate(splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.swapaxes(X_test, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model.evaluate(X_test, [y_valence_test, y_arousal_test]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_valence_test.shape)\n",
    "print(y_arousal_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
