{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xochipilli\\AppData\\Local\\Temp\\ipykernel_11236\\1464098886.py:35: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "from multiprocessing import Value\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *\n",
    "\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "import json\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\" \n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids(physiology_folder)\n",
    "\n",
    "num_cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "def process_files(annotation_file, physiology_file,):\n",
    "    df_annotations = pd.read_csv(annotation_file)\n",
    "    df_physiology = pd.read_csv(physiology_file)\n",
    "    \n",
    "    print(physiology_file)\n",
    "    X, y, numeric_column_indices = preprocess(df_physiology, df_annotations, predictions_cols=['arousal','valence'], aggregate=None, window=[-10000, 10000], partition_window = 1)\n",
    "    # print(X.shape, y.shape)\n",
    "    \n",
    "    save_files(X, y, annotation_file, phys_folder, ann_folder)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# zipped_files = zipped_files[115:]\n",
    "\n",
    "## Process the files using the context manager\n",
    "with parallel_backend('loky', n_jobs=num_cpu_cores // 3):\n",
    "    with tqdm_joblib(tqdm(total=len(zipped_files), desc=\"Files\", leave=False)) as progress_bar:\n",
    "        results = Parallel()(\n",
    "            (delayed(process_files)(ann_file, phys_file) for ann_file, phys_file in zipped_files)\n",
    "        )\n",
    "\n",
    "#for ann_file, phys_file in zipped_files:\n",
    "#    process_files(ann_file, phys_file,)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xochipilli\\Documents\\EPiC-2023-Challenge\\data\\preprocessed\\cleaned_and_prepro_improved\\scenario_2\\fold_0\\preprocessed\\physiology\n"
     ]
    }
   ],
   "source": [
    "print(phys_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\" \n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids(physiology_folder)\n",
    "\n",
    "# Comment this lineas\n",
    "#videos = [0]\n",
    "#subjects = [0,1,11]\n",
    "\n",
    "splits = split_subjects_train_test(subjects, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 18, 31, 20, 13, 34, 32, 17, 2, 9, 15, 45, 0, 11, 7, 24, 5, 28, 41, 43, 27, 30, 1, 29]\n"
     ]
    }
   ],
   "source": [
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape, lstm_units=64, dropout_rate=0.4, kernel_regularizer_l1=0.0032):\n",
    "    input_signal = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(16, 5, padding='same')(input_signal)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = LSTM(lstm_units)(x)\n",
    "\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l1(kernel_regularizer_l1))(x)\n",
    "\n",
    "    return Model(inputs=input_signal, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize an empty dictionary to store the fold results\n",
    "fold_results = {}\n",
    "\n",
    "# Create a directory to save the models\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# Loop through the splits\n",
    "for split_index, split in enumerate(splits):\n",
    "    \n",
    "    X_train, X_test = load_and_concatenate_files(phys_folder, split, videos)\n",
    "    y_train, y_test = load_and_concatenate_files(ann_folder, split, videos)\n",
    "\n",
    "\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "                \n",
    "    # Extract arousal and valence values from y_train and y_test\n",
    "    y_arousal_train = y_train[:, 0]\n",
    "    y_valence_train = y_train[:, 1]\n",
    "    y_arousal_test = y_test[:, 0]\n",
    "    y_valence_test = y_test[:, 1]\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    cnn_lstm_model = create_cnn_lstm_model(input_shape)\n",
    "\n",
    "    # Create separate heads for valence and arousal prediction, using sigmoid activation and scaling the output\n",
    "    valence_output = Dense(1, activation='sigmoid')(cnn_lstm_model.output)\n",
    "    valence_output = Lambda(lambda x: x * 8 + 1, name='valence_output')(valence_output)\n",
    "\n",
    "    arousal_output = Dense(1, activation='sigmoid')(cnn_lstm_model.output)\n",
    "    arousal_output = Lambda(lambda x: x * 8 + 1, name='arousal_output')(arousal_output)\n",
    "\n",
    "    # Combine the model\n",
    "    final_model = Model(inputs=cnn_lstm_model.input, outputs=[valence_output, arousal_output])\n",
    "\n",
    "    # Compile the model\n",
    "    final_model.compile(optimizer='adam',\n",
    "                        loss={'valence_output': 'mse',\n",
    "                            'arousal_output': 'mse'},\n",
    "                        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    # Set up early stopping\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = final_model.fit(X_train, {'valence_output': y_valence_train, 'arousal_output': y_arousal_train},\n",
    "                              validation_split=0.2, epochs=50, batch_size=32,\n",
    "                              callbacks=[early_stopping_callback])\n",
    "\n",
    "    evaluation = final_model.evaluate(X_test, [y_valence_test, y_arousal_test])\n",
    "\n",
    "    # Save the evaluation results in the dictionary\n",
    "    key = f\"split_{split_index}\"\n",
    "    fold_results[key] = {\n",
    "        'valence_output_root_mean_squared_error': evaluation[3],\n",
    "        'arousal_output_root_mean_squared_error': evaluation[4]\n",
    "    }\n",
    "\n",
    "    # Save the trained model for this split\n",
    "    model_path = os.path.join(\"saved_models\", f\"model_split_{split_index}.h5\")\n",
    "    final_model.save(model_path)\n",
    "\n",
    "# Calculate the average of the evaluation results\n",
    "evaluation_sum = np.array([list(rmse_dict.values()) for rmse_dict in fold_results.values()]).sum(axis=0)\n",
    "average_evaluation = evaluation_sum / len(splits)\n",
    "\n",
    "# Save the average evaluation in the dictionary\n",
    "fold_results[\"average\"] = {\n",
    "'valence_output_root_mean_squared_error': average_evaluation[0],\n",
    "'arousal_output_root_mean_squared_error': average_evaluation[1]\n",
    "}\n",
    "\n",
    "# Save the fold_results dictionary to a JSON file\n",
    "with open('results_across_videos.json', 'w') as outfile:\n",
    "    json.dump(fold_results, outfile, indent=4)\n",
    "\n",
    "# Print the final results\n",
    "for fold_key, fold_value in fold_results.items():\n",
    "    print(f\"{fold_key}: {fold_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
