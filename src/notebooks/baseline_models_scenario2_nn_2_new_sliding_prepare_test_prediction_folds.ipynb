{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xochipilli\\AppData\\Local\\Temp\\ipykernel_13080\\1464098886.py:35: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "from multiprocessing import Value\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *\n",
    "\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "import json\n",
    "from keras.layers import Dropout, LSTM\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras.layers import MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 12, 14, 19, 23, 44]\n",
      "[0, 2, 9, 10, 11, 13, 14, 20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdb9f52bde743e288a84f63902c6cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = [0,1,2,3,4]\n",
    "\n",
    "scenario = 2\n",
    "\n",
    "for fold in folds:\n",
    "\n",
    "    scenario_folder = f\"../data/preprocessed/cleaned_and_prepro_improved/scenario_{scenario}/\"\n",
    "\n",
    "\n",
    "    phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "    annotations_folder = f\"../data/raw/scenario_2/fold_{fold}/test/annotations/\"\n",
    "    physiology_folder = f\"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_{fold}/test/physiology/\" \n",
    "    zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "    subjects, videos = get_subs_vids(physiology_folder)\n",
    "    print(subjects)\n",
    "    print(videos)\n",
    "    \n",
    "\n",
    "    num_cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "    def process_files(annotation_file, physiology_file,):\n",
    "        df_annotations = pd.read_csv(annotation_file)\n",
    "        df_physiology = pd.read_csv(physiology_file)\n",
    "        \n",
    "        print(physiology_file)\n",
    "        X, y, numeric_column_indices = preprocess(df_physiology, df_annotations, predictions_cols=['arousal','valence'], aggregate=None, window=[-10000, 10000], partition_window = 1)\n",
    "        \n",
    "        save_files(X, y, annotation_file, phys_folder, ann_folder)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    ## Process the files using the context manager\n",
    "    with parallel_backend('loky', n_jobs=num_cpu_cores // 3):\n",
    "        with tqdm_joblib(tqdm(total=len(zipped_files), desc=\"Files\", leave=False)) as progress_bar:\n",
    "            results = Parallel()(\n",
    "                (delayed(process_files)(ann_file, phys_file) for ann_file, phys_file in zipped_files)\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [31, 24, 7, 45], 'test': [32, 20]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape, lstm_units=64, dropout_rate=0.4, kernel_regularizer_l1=0.0032):\n",
    "    input_signal = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(16, 5, padding='same')(input_signal)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = LSTM(lstm_units)(x)\n",
    "\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l1(kernel_regularizer_l1))(x)\n",
    "\n",
    "    return Model(inputs=input_signal, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 5, 7, 9, 11, 13, 15, 17, 18, 20, 24, 26, 27, 28, 29, 30, 31, 32, 34, 41, 43, 45]\n",
      "[0, 2, 9, 10, 11, 13, 14, 20]\n",
      "{'train': [43, 11, 0, 13, 32, 2, 34, 15, 29, 9, 5, 24, 45, 31, 41, 26], 'test': [28, 18, 30, 17, 7, 27, 1, 20]}\n",
      "[0, 2, 3, 5, 7, 9, 12, 13, 14, 15, 18, 19, 20, 23, 24, 27, 29, 30, 31, 32, 34, 43, 44, 45]\n",
      "[0, 2, 9, 10, 11, 13, 14, 20]\n",
      "{'train': [27, 32, 14, 30, 19, 20, 18, 9, 29, 23, 0, 2, 13, 43, 5, 24], 'test': [34, 15, 7, 12, 44, 31, 3, 45]}\n",
      "[0, 1, 2, 3, 7, 11, 12, 13, 14, 15, 17, 18, 19, 20, 23, 24, 26, 28, 31, 32, 41, 43, 44, 45]\n",
      "[0, 2, 9, 10, 11, 13, 14, 20]\n",
      "{'train': [2, 17, 11, 14, 43, 12, 7, 41, 18, 28, 24, 15, 19, 3, 1, 20], 'test': [26, 0, 23, 31, 44, 13, 45, 32]}\n",
      "[1, 3, 5, 7, 9, 11, 12, 14, 17, 19, 20, 23, 24, 26, 27, 28, 29, 30, 31, 32, 34, 41, 44, 45]\n",
      "[0, 2, 9, 10, 11, 13, 14, 20]\n",
      "{'train': [24, 41, 34, 32, 26, 23, 9, 14, 19, 31, 30, 17, 11, 7, 3, 5], 'test': [1, 29, 44, 20, 28, 12, 27, 45]}\n",
      "[0, 1, 2, 3, 5, 9, 11, 12, 13, 14, 15, 17, 18, 19, 23, 26, 27, 28, 29, 30, 34, 41, 43, 44]\n",
      "[0, 2, 9, 10, 11, 13, 14, 20]\n",
      "{'train': [28, 18, 3, 17, 19, 1, 13, 5, 12, 23, 0, 34, 9, 41, 44, 14], 'test': [15, 2, 29, 11, 27, 30, 43, 26]}\n"
     ]
    }
   ],
   "source": [
    "for fold in folds:\n",
    "\n",
    "    scenario_folder = f\"../data/preprocessed/cleaned_and_prepro_improved/scenario_{scenario}/\"\n",
    "\n",
    "\n",
    "    phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "    annotations_folder = f\"../data/raw/scenario_2/fold_{fold}/test/annotations/\"\n",
    "    physiology_folder = f\"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_{fold}/train/physiology/\" \n",
    "\n",
    "    subjects, videos = get_subs_vids(physiology_folder)\n",
    "    print(subjects)\n",
    "    print(videos)\n",
    "    \n",
    "    splits = split_subjects_train_test_final_model(subjects)\n",
    "    print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize an empty dictionary to store the fold results\n",
    "fold_results = {}\n",
    "\n",
    "# def save_test_data(y_pred, output_folder, y_test_file, test=False, y_test=None):\n",
    "#     y_test_file = y_test_file.replace('npy', 'csv')\n",
    "    \n",
    "#     if test:\n",
    "#         df = pd.read_csv(y_test_file)\n",
    "#         df['valence'] = y_pred[:, 0]\n",
    "#         df['arousal'] = y_pred[:, 1]  \n",
    "#     else:\n",
    "#         df = pd.DataFrame(y_test, columns=['valence', 'arousal'])\n",
    "#         df['valence_pred'] = y_pred[:, 1]\n",
    "#         df['arousal_pred'] = y_pred[:, 0] \n",
    "#         df['time'] = pd.read_csv(y_test_file).tail(df.shape[0])['time'].values         \n",
    "    \n",
    "#     df.to_csv(os.path.join(output_folder, os.path.basename(y_test_file)), index=False)\n",
    "#     return None\n",
    "\n",
    "scenario = 2\n",
    "\n",
    "# Process each fold\n",
    "folds = [0,1,2,3,4]\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    output_folder = f\"data/predictions/scenario_2/fold_{fold}\"\n",
    "    scenario_folder = f\"../data/preprocessed/cleaned_and_prepro_improved/scenario_{scenario}/\"\n",
    "    phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "    \n",
    "    annotations_folder = f\"../data/raw/scenario_2/fold_{fold}/test/annotations/\"\n",
    "    physiology_folder = f\"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_{fold}/train/physiology/\" \n",
    "    zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "    subjects, videos = get_subs_vids(physiology_folder)\n",
    "    print(subjects)\n",
    "    print(videos)\n",
    "    \n",
    "    splits = split_subjects_train_test_final_model(subjects)\n",
    "    print(splits)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    subjects, videos = get_subs_vids(phys_folder)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # X_train = []\n",
    "    # for train_test, subs in train_test_split.items():\n",
    "    #     for v in videos:\n",
    "    #         for sub in subs:\n",
    "    #             file_path = os.path.join(base_path, f\"sub_{sub}_vid_{v}.npy\")\n",
    "    #             if os.path.exists(file_path):\n",
    "    #                 data = np.load(file_path)\n",
    "    #                     X_train.append(data)\n",
    "\n",
    "    X_train, x_test = load_and_concatenate_files(phys_folder, splits, videos)\n",
    "    y_train, y_test = load_and_concatenate_files(ann_folder, splits, videos)\n",
    "\n",
    "    X_train = np.transpose(X_train, (1, 0, 2))\n",
    "    X_test = np.transpose(X_test, (1, 0, 2))\n",
    "\n",
    "                \n",
    "    # Extract arousal and valence values from y_train and y_test\n",
    "    y_arousal_train = y_train[:, 0]\n",
    "    y_valence_train = y_train[:, 1]\n",
    "    y_arousal_test = y_test[:, 0]\n",
    "    y_valence_test = y_test[:, 1]\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    cnn_lstm_model = create_cnn_lstm_model(input_shape)\n",
    "\n",
    "    # Create separate heads for valence and arousal prediction, using sigmoid activation and scaling the output\n",
    "    valence_output = Dense(1, activation='sigmoid')(cnn_lstm_model.output)\n",
    "    valence_output = Lambda(lambda x: x * 8 + 1, name='valence_output')(valence_output)\n",
    "\n",
    "    arousal_output = Dense(1, activation='sigmoid')(cnn_lstm_model.output)\n",
    "    arousal_output = Lambda(lambda x: x * 8 + 1, name='arousal_output')(arousal_output)\n",
    "\n",
    "    # Combine the model\n",
    "    final_model = Model(inputs=cnn_lstm_model.input, outputs=[valence_output, arousal_output])\n",
    "\n",
    "    # Compile the model\n",
    "    final_model.compile(optimizer='adam',\n",
    "                        loss={'valence_output': 'mse',\n",
    "                            'arousal_output': 'mse'},\n",
    "                        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    # Set up early stopping\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = final_model.fit(X_train, {'valence_output': y_valence_train, 'arousal_output': y_arousal_train},\n",
    "                                validation_split=0.2, epochs=5, batch_size=32,\n",
    "                                callbacks=[early_stopping_callback])\n",
    "\n",
    "    # Predict\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    # Save predictions\n",
    "    save_test_data(y_pred, output_folder, ann_folder)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_valence = np.sqrt(np.mean((y_pred[:, 0] - y_test[:, 0]) ** 2))\n",
    "    rmse_arousal = np.sqrt(np.mean((y_pred[:, 1] - y_test[:, 1]) ** 2))\n",
    "\n",
    "    # Save the RMSE in the fold_results dictionary\n",
    "    fold_results[f\"fold_{fold}\"] = {\n",
    "        'valence_output_root_mean_squared_error': rmse_valence,\n",
    "        'arousal_output_root_mean_squared_error': rmse_arousal\n",
    "    }\n",
    "\n",
    "# Save the fold_results dictionary to a JSON file\n",
    "with open('results_LSTM_folds.json', 'w') as outfile:\n",
    "    json.dump(fold_results, outfile, indent=4)\n",
    "\n",
    "# Print the final results\n",
    "for fold_key, fold_value in fold_results.items():\n",
    "    print(f\"{fold_key}: {fold_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
