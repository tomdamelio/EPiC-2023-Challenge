{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "from multiprocessing import Value\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# all the functions from helpers.py\n",
    "from helpers_scenario2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/\"\n",
    "fold = 0\n",
    "\n",
    "phys_folder, ann_folder = create_folder_structure(scenario_folder, fold)\n",
    "\n",
    "annotations_folder = '../data/raw/scenario_2/fold_0/train/annotations/'\n",
    "physiology_folder = \"../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/\"\n",
    "zipped_files = zip_csv_files(annotations_folder, physiology_folder)\n",
    "\n",
    "subjects, videos = get_subs_vids('../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology')\n",
    "\n",
    "splits = splits = split_subjects_train_test(subjects, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Define aggregate metric combinations\n",
    "aggregate_combinations = [\n",
    "    # ['enlarged'],\n",
    "    # ['mean'],\n",
    "    # ['std'],\n",
    "    # ['max'],\n",
    "    # ['min'],\n",
    "    # ['mean', 'std'],\n",
    "    # ['mean', 'max'],\n",
    "    # ['mean', 'min'],\n",
    "    # ['std', 'max'],\n",
    "    # ['std', 'min'],\n",
    "    # ['max', 'min'],\n",
    "    ['mean', 'std', 'max', 'min']\n",
    "]\n",
    "\n",
    "# Define models and hyperparameters\n",
    "models_hyperparameters = [\n",
    "    # (LinearRegression, {}),\n",
    "    # (SVR, {\n",
    "    #     'kernel': ['linear', 'rbf'],\n",
    "    #     'C': [0.1, 1, 10],\n",
    "    #     'epsilon': [0.1, 1],\n",
    "    #     'gamma': ['scale', 'auto'],  # Only used for 'rbf' kernel\n",
    "    # }),\n",
    "    (RandomForestRegressor, {\n",
    "        'n_estimators': [100],#[50, 100],\n",
    "        'max_depth': [None],#[10, None],\n",
    "        'min_samples_split': [5],#[2, 5],\n",
    "        'min_samples_leaf': [1],\n",
    "    }),\n",
    "    # (XGBRegressor, {\n",
    "    #     'n_estimators': [50, 100],\n",
    "    #     'max_depth': [6, 10],\n",
    "    #     'learning_rate': [0.01, 0.1],\n",
    "    #     'subsample': [0.5, 0.8],\n",
    "        # 'colsample_bytree': [0.5, 0.8],\n",
    "        # 'reg_alpha': [0, 0.1],\n",
    "        # 'reg_lambda': [0.1, 1],\n",
    "    # }),\n",
    "]\n",
    "\n",
    "windows = [[-10000, 10000], [-5000, 5000], [-2000,2000],]\n",
    "partitions = [1, 2, 3, 5]\n",
    "\n",
    "# Initialize an empty DataFrame for the best results and a dictionary for all results\n",
    "best_results_df = pd.DataFrame()\n",
    "all_results = {}\n",
    "\n",
    "\n",
    "def process_files(annotation_file, physiology_file,):\n",
    "    df_annotations = pd.read_csv(annotation_file)\n",
    "    df_physiology = pd.read_csv(physiology_file)\n",
    "    \n",
    "    print(physiology_file)\n",
    "    X, y, numeric_column_indices = preprocess(df_physiology, df_annotations,  predictions_cols=['arousal','valence'], aggregate=['mean','min'], window=[-10000, 5000])\n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    save_files(X, y, annotation_file, phys_folder, ann_folder)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_video(subject, models_hyperparameters, splits, phys_folder, ann_folder, window, partition_window, ):\n",
    "    results = []\n",
    "\n",
    "    for model, hyperparameters in models_hyperparameters:\n",
    "        for hp_set in itertools.product(*hyperparameters.values()):\n",
    "            hp_dict = dict(zip(hyperparameters.keys(), hp_set))\n",
    "\n",
    "            model_name = model.__name__\n",
    "\n",
    "            rmses = []\n",
    "            for split in splits:\n",
    "\n",
    "                X_train, X_test = load_and_concatenate_files(phys_folder, split, subject)\n",
    "                y_train, y_test = load_and_concatenate_files(ann_folder, split, subject)\n",
    "\n",
    "                rmse = time_series_cross_validation_with_hyperparameters(\n",
    "                    X_train, X_test, y_train, y_test, model, hp_dict, n_jobs=-1,\n",
    "                    numeric_column_indices=np.array(range(X_train.shape[1])))\n",
    "\n",
    "                rmses.append(rmse)\n",
    "\n",
    "            average_rmse = np.mean(rmses, axis=0)\n",
    "            print(f\"Average Root Mean Squared Error per output: {average_rmse}. \")\n",
    "\n",
    "            if y_train.ndim > 1 and y_train.shape[1] > 1:\n",
    "                # Unpack the average_rmse array into separate keys in the result dictionary\n",
    "                result = {\n",
    "                    'model': model_name,\n",
    "                    'hyperparameters': hyperparameters,\n",
    "                    'aggregate': ['mean','min'],\n",
    "                    'average_rmse_arousal': average_rmse[0],\n",
    "                    'average_rmse_valence': average_rmse[1],\n",
    "                    'window': window,\n",
    "                    'partition_window': partition_window\n",
    "                    \n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    'model': model_name,\n",
    "                    'hyperparameters': hyperparameters,\n",
    "                    'aggregate': ['mean','min'],\n",
    "                    'average_rmse': average_rmse,\n",
    "                    'window': window,\n",
    "                    'partition_window': partition_window\n",
    "                }\n",
    "            results.append(result)\n",
    "            \n",
    "    # # Update the all_results dictionary\n",
    "    # all_results[f\"{subject}\"] = results\n",
    "\n",
    "    # # Save all_results as JSON\n",
    "    # with open('../results/scenario_3/clean_all_results_shallow_models_window.json', 'w') as f:\n",
    "    #     json.dump(all_results, f, default=str, indent=4)\n",
    "    return results\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c84271d7bd4acb81ecc5657558eb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Windows:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b171fcbfd5d74aa3aa148571b3cfc91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Partitions:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfc2d85ce7d47de9798106e994b3316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_26_vid_11.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_31_vid_11.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_7_vid_11.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_15_vid_10.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_43_vid_20.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_45_vid_0.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_20_vid_2.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_34_vid_10.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_1_vid_2.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_41_vid_2.csv../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_34_vid_9.csv\n",
      "\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_31_vid_9.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_29_vid_2.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_11_vid_2.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_29_vid_20.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_27_vid_9.csv../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_32_vid_9.csv\n",
      "\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_5_vid_14.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_26_vid_14.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_26_vid_0.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_41_vid_13.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_15_vid_20.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_17_vid_20.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_2_vid_0.csv\n",
      "(1001, 26) (1001, 2)\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_1_vid_14.csv\n",
      "(1001, 26) (1001, 2)\n",
      "(1001, 26) (1001, 2)\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_29_vid_14.csv\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_18_vid_9.csv\n",
      "(1503, 26) (1503, 2)\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_31_vid_20.csv\n",
      "(1503, 26) (1503, 2)\n",
      "../data/preprocessed/cleaned_and_prepro_improved/scenario_2/fold_0/train/physiology/sub_24_vid_13.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39m'\u001b[39m\u001b[39mmultiprocessing\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39mnum_cpu_cores \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m tqdm_joblib(tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(zipped_files), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFiles\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)) \u001b[39mas\u001b[39;00m progress_bar:\n\u001b[0;32m---> 12\u001b[0m         results \u001b[39m=\u001b[39m Parallel()(\n\u001b[1;32m     13\u001b[0m             (delayed(process_files)(ann_file, phys_file) \u001b[39mfor\u001b[39;49;00m ann_file, phys_file \u001b[39min\u001b[39;49;00m zipped_files)\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     16\u001b[0m total_videos \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(videos)\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39m'\u001b[39m\u001b[39mmultiprocessing\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m num_cpu_cores \u001b[39m-\u001b[39m \u001b[39m5\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with open(r'../results/scenario_2/clean_all_results_shallow_models_window.json', 'r') as f:\n",
    "#     all_results = json.load(f)\n",
    "    \n",
    "for window in tqdm(windows, desc=\"Windows\"):\n",
    "    for partition_window in tqdm(partitions[1:], desc=\"Partitions\", leave=False):\n",
    "        \n",
    "        zipped_files = zip_csv_files(annotations_folder, physiology_folder)        \n",
    "        \n",
    "        # Process the files using the context manager\n",
    "        with parallel_backend('multiprocessing', n_jobs=num_cpu_cores // 2):\n",
    "            with tqdm_joblib(tqdm(total=len(zipped_files), desc=\"Files\", leave=False)) as progress_bar:\n",
    "                results = Parallel()(\n",
    "                    (delayed(process_files)(ann_file, phys_file) for ann_file, phys_file in zipped_files)\n",
    "                )\n",
    "\n",
    "        total_videos = len(videos)\n",
    "        with parallel_backend('multiprocessing', n_jobs= num_cpu_cores - 5):\n",
    "            with tqdm_joblib(tqdm(total=total_videos, desc=\"Videos\", leave=False)) as progress_bar:\n",
    "                all_subject_results = Parallel()(\n",
    "                    (delayed(process_video)(video, models_hyperparameters, splits, phys_folder, ann_folder, window, partition_window) \n",
    "                     for video in videos)\n",
    "                )\n",
    "\n",
    "\n",
    "        # Combine results for all subjects\n",
    "        for subject_idx, subject in enumerate(subjects):\n",
    "            subject_results = all_subject_results[subject_idx]\n",
    "            all_results[f\"{subject}\"] = subject_results\n",
    "\n",
    "            # Save all_results as JSON\n",
    "            with open('../results/scenario_2/clean_all_results_shallow_models_window.json', 'w') as f:\n",
    "                json.dump(all_results, f, default=str, indent=4)\n",
    "\n",
    "                    \n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    best_result_output_1 = results_df.loc[results_df['average_rmse_arousal'].idxmin()]\n",
    "    best_result_output_2 = results_df.loc[results_df['average_rmse_valence'].idxmin()]\n",
    "\n",
    "    # Concatenate the best results for each output variable to the best_results_df\n",
    "    best_results_df =pd.concat([best_results_df, best_result_output_1.to_frame().T, best_result_output_2.to_frame().T], ignore_index=True)\n",
    "\n",
    "    # Save best_results_df as CSV\n",
    "    best_results_df.to_csv('../results/scenario_2/clean_shallow_models_best_result_window.csv', index=False)\n",
    "\n",
    "print(\"\\nThe best combination of features and hyperparameters for each file pair is:\")\n",
    "print(best_results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
